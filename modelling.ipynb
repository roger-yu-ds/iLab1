{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from rfpimp import permutation_importances\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from UTILS import utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path.cwd().joinpath('OUTPUT')\n",
    "column_dir = output_dir.joinpath('COLUMNS')\n",
    "config_dir = Path.cwd().joinpath('CONFIG')\n",
    "model_dir = output_dir.joinpath('MODELS')\n",
    "image_dir = output_dir.joinpath('IMAGES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd().joinpath('OUTPUT').joinpath('df_merged')\n",
    "with open(data_dir, 'rb') as infile:\n",
    "    df_merged = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude the columns that are not included, according to `/CONFIG/ml_columns.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_columns_df = utils.load_data(\n",
    "    config_dir,\n",
    "    'ml_columns.csv',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_columns = ml_columns_df.query('use == 1')['columns'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_merged.copy(deep=True)\n",
    "\n",
    "for filename in column_dir.iterdir():\n",
    "    if str(filename.stem) in ml_columns:\n",
    "        print(f'merging {filename.stem}')\n",
    "        df_features = utils.add_column(df=df_features,\n",
    "                                       data_dir=column_dir,\n",
    "                                       filename=str(filename.stem)\n",
    "                                      )\n",
    "\n",
    "# 'student_comment_pos_tags' is actually a dataframe, not a series. So the column name is not 'student_comment_pos_tags'.        \n",
    "df_features = utils.add_column(df=df_features,\n",
    "                               data_dir=column_dir,\n",
    "                               filename='student_comment_pos_tags'\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_features[ml_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset the Data and Set the Data Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Keep only the columns that would be usable by the machine learning algorithms; the mapping is stored in `/CONFIG/ml_columns.csv`.\n",
    "2. Keep only the rows that have a student rating.\n",
    "3. Configure the data types according to `/CONFIG/mapping_column_types_extended.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_features[df_features.student_rating_numeric.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_subset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_column_types_extended = utils.load_data(\n",
    "    config_dir,\n",
    "    'mapping_column_types_extended.csv'\n",
    "    ).set_index('columns')\n",
    "\n",
    "mapping_column_types_extended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.qualifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = (df_subset\n",
    "             .apply(lambda x: utils.map_column_dtype(x, mapping_column_types_extended))\n",
    "            )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation of missing values is required because the algorithms in `sklearn` does not handle missing values properly. A simple imputation scheme is employed whereby a new category \"missing\" is imputed for categorical variables and a 0 is imputed for numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = utils.simple_impute(df_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the Categorical Variable to Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some algorithms in the `sklearn` package cannot deal with string-valued categorical variables. Therefore we will now convert such variables into one dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = df_imputed.select_dtypes(include='category').columns.tolist()\n",
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df_imputed,\n",
    "                            drop_first=True)\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_object(\n",
    "    df_dummies,\n",
    "    'df_dummies',\n",
    "    output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Into X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `y` is a column that contains the values that we want to predict, i.e. the `student_rating`. The `X` is a set of columns that are used to predict the `y`, e.g. waiting time, tutor age, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_dummies.iloc[:, 1:], df_dummies.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split such that the proportions of the values of target variable, i.e. `student_rating_fixed` is maintained after the split. This is called stratification and has been shown to produce a better results. In the `test_train_split` function, the parameter is `stratify`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.calc_percentage_counts(\n",
    "    df_dummies.student_rating_numeric,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=1,\n",
    "    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.calc_percentage_counts(\n",
    "    y_train,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table, it can be seen that the counts are about 70% of the previous table, however, the proportions are maintained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training uses `RandomizedSearchCV`, which searches a random sample of the hyperparameters. The score used is the [`neg_mean_squared_error`](neg_mean_squared_error) negative mean square error, which is scale dependent and is directly interpretable, the negation is purely because the `RandomizedSearchCV` maximises a criteria, rather than minimises it, so the smaller the negative number the better performing the algorithm is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(f'Training started at {start_time}.')\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "regressor = RandomForestRegressor()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 5, \n",
    "                               cv = 5, \n",
    "                               verbose=2, \n",
    "                               random_state=42, \n",
    "                               n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_test, y_test)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f'Training completed at {end_time}.')\n",
    "run_time = end_time - start_time\n",
    "print(f'Runtime was {run_time}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random = utils.load_data(\n",
    "    model_dir,\n",
    "    'rf_random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(f'Training started at {start_time}.')\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [550, 650]\n",
    "max_features = ['sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [80, 100]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [9, 11]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [4, 5]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [False]\n",
    "# Create the random grid\n",
    "grid = {'n_estimators': n_estimators,\n",
    "        'max_features': max_features,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'bootstrap': bootstrap}\n",
    "\n",
    "regressor = RandomForestRegressor()\n",
    "\n",
    "rf_grid = GridSearchCV(estimator = regressor, \n",
    "                       param_grid = grid, \n",
    "                       cv = 5, \n",
    "                       verbose=2, \n",
    "                       n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f'Training completed at {end_time}.')\n",
    "run_time = end_time - start_time\n",
    "print(f'Runtime was {run_time}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_object(\n",
    "    rf_grid,\n",
    "    'rf_grid',\n",
    "    model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rf_grid.best_estimator_.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = utils.load_data(\n",
    "    model_dir,\n",
    "    'rf_grid'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = utils.load_data(\n",
    "    model_dir,\n",
    "    'rf_grid'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rf_grid.best_estimator_.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',\n",
    "                                                                        ascending=False)\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,30))\n",
    "\n",
    "sns.barplot(x='importance',\n",
    "            y=feature_importances.index,\n",
    "            data=feature_importances,\n",
    "           )\n",
    "\n",
    "# Saving\n",
    "model = 'rf'\n",
    "extension = 'png'\n",
    "filename = f'variable_importance_{model}.{extension}'\n",
    "filepath = image_dir.joinpath(filename)\n",
    "plt.tight_layout()\n",
    "plt.savefig(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "\n",
    "sns.barplot(x='importance',\n",
    "            y=feature_importances.head().index,\n",
    "            data=feature_importances.head(),\n",
    "           )\n",
    "\n",
    "# Saving\n",
    "model = 'rf'\n",
    "extension = 'png'\n",
    "filename = f'variable_importance_{model}_top_5.{extension}'\n",
    "filepath = image_dir.joinpath(filename)\n",
    "plt.tight_layout()\n",
    "plt.savefig(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(f'Training started at {start_time}.')\n",
    "\n",
    "regressor = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "params = {\n",
    "    'n_estimators': stats.randint(3, 40),\n",
    "    'max_depth': stats.randint(3, 40),\n",
    "    'learning_rate': stats.uniform(0.05, 0.4),\n",
    "    'colsample_bytree': stats.beta(10, 1),\n",
    "    'subsample': stats.beta(10, 1),\n",
    "    'gamma': stats.uniform(0, 10),\n",
    "    'reg_alpha': stats.expon(0, 50),\n",
    "    'min_child_weight': stats.expon(0, 50),\n",
    "}\n",
    "cv_results = RandomizedSearchCV(estimator = regressor, \n",
    "                                param_distributions = params, \n",
    "                                n_iter = 20, \n",
    "                                cv = 5, \n",
    "                                verbose=2, \n",
    "                                random_state=42, \n",
    "                                n_jobs = -1\n",
    "                               )\n",
    "\n",
    "cv_results.fit(X_train, y_train)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f'Training completed at {end_time}.')\n",
    "\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f'Runtime was {run_time}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_object(cv_results, \n",
    "                  'xgb_regressor_randomizedsearchcv', \n",
    "                  model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cv_results.best_estimator_.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Regression Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Number of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,25))\n",
    "xgb.plot_importance(xgb_regressor_randomizedsearchcv.best_estimator_,\n",
    "                    ax=ax)\n",
    "\n",
    "# Saving\n",
    "model = 'xgb'\n",
    "extension = 'png'\n",
    "filename = f'variable_importance_{model}.{extension}'\n",
    "filepath = image_dir.joinpath(filename)\n",
    "plt.tight_layout()\n",
    "plt.savefig(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (10,3)\n",
    "max_num_features = 10\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "xgb.plot_importance(xgb_regressor_randomizedsearchcv.best_estimator_,\n",
    "                    max_num_features=max_num_features,\n",
    "                    ax=ax)\n",
    "\n",
    "# Saving\n",
    "model = 'xgb'\n",
    "extension = 'png'\n",
    "\n",
    "filename = f'~variable_importance_{model}_top_{max_num_features}.{extension}'\n",
    "filepath = image_dir.joinpath(filename)\n",
    "plt.tight_layout()\n",
    "plt.savefig(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (Principle Components Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is known to take a long time, especially if there are a lot of features, so it would be more feasible to apply some dimensionality reduction techniques to reduce the 100 columns to something more manageble. To that end PCA is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "\n",
    "plt.figure(1, figsize=(20, 10))\n",
    "plt.clf()\n",
    "plt.axes([.2, .2, .7, .7])\n",
    "plt.plot(pca.explained_variance_, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_variance_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow occurs at very low n_components. Which means that the data set is highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(f'SVR started at {start_time}.')\n",
    "\n",
    "pipe_svr = make_pipeline(PCA(n_components=5),\n",
    "                         StandardScaler(),\n",
    "                         SVR())\n",
    "pipe_svr.fit(X_train, y_train)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f'Training completed at {end_time}.')\n",
    "\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f'Runtime was {run_time}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[:, :50].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10,000 rows → 15 seconds\n",
    "20,000 rows → 3 minutes\n",
    "20,000 rows with 50 columns → 1:24\n",
    "20,000 rows with 10 columns → 41 seconds\n",
    "20,000 rows with 5 columns → 16 seconds\n",
    "100,000 rows with 5 columns → 16 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small = X_train.sample(100000).iloc[:, :5]\n",
    "y_train_small = y_train.sample(100000)\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f'SVR started at {start_time}.')\n",
    "\n",
    "pipe_svr = make_pipeline(PCA(),\n",
    "                         StandardScaler(),\n",
    "                         SVR(),\n",
    "\n",
    "params = {\n",
    "    'pca__n_components': [3, 4, 5, 6],\n",
    "    'svr__kernel': ['rbf', 'poly'],\n",
    "    'svr__C': [1, 5, 10, 20, 40],\n",
    "    'svr__degree': [2, 3, 4, 5, 6]\n",
    "}\n",
    "\n",
    "cv_results = RandomizedSearchCV(estimator=pipe_svr,\n",
    "                                param_distributions=tributions=params,\n",
    "                                n_iter=12,\n",
    "                                cv=5,\n",
    "                                verbose=2,\n",
    "                                random_state=42,\n",
    "                                n_jobs=-1\n",
    "                               )\n",
    "\n",
    "cv_results.fit(X_train, y_train)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f'Training completed at {end_time}.')\n",
    "\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f'Runtime was {run_time}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `VotingRegressor` pre-fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the hyperparameters of the two models using `RandomizedSearchCV`, the results can be combined to potentially create a better performing regressor. This is done by using the `VotingRegressor` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(f'SVR started at {start_time}.')\n",
    "\n",
    "vr = VotingRegressor([('lr', rf_grid.best_estimator_), \n",
    "                      ('rf', xgb_regressor_randomizedsearchcv.best_estimator_)])\n",
    "\n",
    "vr.fit(X_train, y_train)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f'Training completed at {end_time}.')\n",
    "\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f'Runtime was {run_time}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_object(\n",
    "    vr,\n",
    "    'vr_xgb_rf_prefitted',\n",
    "    model_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `VotingRegressor` pre-fitted RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = vr.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `VotingRegressor` Unfitted Using `RandomizedSearchCV` `best_params_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfitted models but using the best parameters obtained from the `RandomizedSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(f'SVR started at {start_time}.')\n",
    "\n",
    "vr = VotingRegressor([('rf', RandomForestRegressor(n_estimators=600,\n",
    "                                                   min_samples_split=10,\n",
    "                                                   min_samples_leaf=4,\n",
    "                                                   max_features='sqrt',\n",
    "                                                   max_depth=90,\n",
    "                                                   bootstrap=False)), \n",
    "                      ('xgb', xgb.XGBRegressor(colsample_bytree=0.9252155845351104,\n",
    "                                               gamma=1.5601864044243652,\n",
    "                                               learning_rate=0.11239780813448107,\n",
    "                                               max_depth=13,\n",
    "                                               min_child_weight=30.73980825409684,\n",
    "                                               n_estimators=38,\n",
    "                                               reg_alpha=7.708098373328053,\n",
    "                                               subsample=0.9937572296628479)\n",
    "                      )])\n",
    "\n",
    "vr.fit(X_train, y_train)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f'Training completed at {end_time}.')\n",
    "\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f'Runtime was {run_time}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_object(\n",
    "    vr,\n",
    "    'vr_xgb_rf_unfitted',\n",
    "    model_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `VotingRegressor` un-fitted RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = vr.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `VotingRegressor` Unfitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfitted fresh estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(f'SVR started at {start_time}.')\n",
    "\n",
    "vr = VotingRegressor([('rf', RandomForestRegressor()), \n",
    "                      ('xgb', xgb.XGBRegressor()\n",
    "                      )])\n",
    "\n",
    "vr.fit(X_train, y_train)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f'Training completed at {end_time}.')\n",
    "\n",
    "run_time = end_time - start_time\n",
    "\n",
    "print(f'Runtime was {run_time}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_object(\n",
    "    vr,\n",
    "    'vr_xgb_rf_unfitted_fresh',\n",
    "    model_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `VotingRegressor` un-fitted RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = vr.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "print(f'RMSE: {rmse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
